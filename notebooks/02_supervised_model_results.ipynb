{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ Supervised Model - Complete Pipeline Visualization\n",
        "\n",
        "Visualizes the complete supervised model training pipeline:\n",
        "1. Model Comparison (Linear, Ridge, Random Forest)\n",
        "2. Best Model Selection\n",
        "3. Hyperparameter Optimization\n",
        "4. Final Model Performance\n",
        "\n",
        "This notebook mirrors the workflow in `main.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.movie_data_loader import MovieDataLoader\n",
        "from models.supervised.feature_engineering import FeatureEngineer\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "loader = MovieDataLoader()\n",
        "df = loader.get_supervised_data()\n",
        "\n",
        "# Engineer features\n",
        "print(\"Engineering features...\")\n",
        "engineer = FeatureEngineer()\n",
        "df = engineer.create_all_features(df)\n",
        "X, y = engineer.prepare_features_and_target(df)\n",
        "X_scaled = engineer.fit_transform(X)\n",
        "\n",
        "print(f\"\\nData ready: {len(df):,} movies, {len(engineer.feature_names)} features\")\n",
        "print(f\"Target range: {y.min():.1f} - {y.max():.1f}\")\n",
        "print(f\"Target mean: {y.mean():.2f} Â± {y.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train and Compare All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.supervised.trainer import QualityModelTrainer\n",
        "\n",
        "# Models to test\n",
        "models_to_test = {\n",
        "    'Linear Regression': 'linear',\n",
        "    'Ridge Regression': 'ridge',\n",
        "    'Random Forest': 'random_forest'\n",
        "}\n",
        "\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "print(\"Training models...\\n\")\n",
        "for model_name, model_type in models_to_test.items():\n",
        "    print(f\"Training {model_name}...\", end=\" \")\n",
        "    trainer = QualityModelTrainer(model_type=model_type)\n",
        "    metrics = trainer.train_and_evaluate(X_scaled, y.values)\n",
        "    results[model_name] = metrics\n",
        "    trained_models[model_name] = trainer\n",
        "    print(f\"RÂ²={metrics['test_r2']:.4f}\")\n",
        "\n",
        "print(\"\\nAll models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(results).T\n",
        "comparison_df = comparison_df[['test_r2', 'test_rmse', 'test_mae']]\n",
        "comparison_df.columns = ['RÂ²', 'RMSE', 'MAE']\n",
        "\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string())\n",
        "\n",
        "# Find best model\n",
        "best_model_name = comparison_df['RÂ²'].idxmax()\n",
        "print(f\"\\nBest Model: {best_model_name} (RÂ²={comparison_df.loc[best_model_name, 'RÂ²']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# RÂ² comparison\n",
        "colors = ['#2ecc71' if model == best_model_name else '#95a5a6' for model in comparison_df.index]\n",
        "axes[0].bar(range(len(comparison_df)), comparison_df['RÂ²'], color=colors, edgecolor='black')\n",
        "axes[0].set_xticks(range(len(comparison_df)))\n",
        "axes[0].set_xticklabels(comparison_df.index, rotation=15, ha='right')\n",
        "axes[0].set_ylabel('RÂ² Score')\n",
        "axes[0].set_title('RÂ² Score Comparison (Higher is Better)', fontweight='bold')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['RÂ²']):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# RMSE comparison\n",
        "colors = ['#2ecc71' if model == best_model_name else '#95a5a6' for model in comparison_df.index]\n",
        "axes[1].bar(range(len(comparison_df)), comparison_df['RMSE'], color=colors, edgecolor='black')\n",
        "axes[1].set_xticks(range(len(comparison_df)))\n",
        "axes[1].set_xticklabels(comparison_df.index, rotation=15, ha='right')\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].set_title('RMSE Comparison (Lower is Better)', fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['RMSE']):\n",
        "    axes[1].text(i, v + 0.2, f'{v:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# MAE comparison\n",
        "colors = ['#2ecc71' if model == best_model_name else '#95a5a6' for model in comparison_df.index]\n",
        "axes[2].bar(range(len(comparison_df)), comparison_df['MAE'], color=colors, edgecolor='black')\n",
        "axes[2].set_xticks(range(len(comparison_df)))\n",
        "axes[2].set_xticklabels(comparison_df.index, rotation=15, ha='right')\n",
        "axes[2].set_ylabel('MAE')\n",
        "axes[2].set_title('MAE Comparison (Lower is Better)', fontweight='bold')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['MAE']):\n",
        "    axes[2].text(i, v + 0.2, f'{v:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.supervised.optimizer import optimize_model\n",
        "\n",
        "# Get best model type\n",
        "best_model_type = models_to_test[best_model_name]\n",
        "best_metrics = results[best_model_name]\n",
        "\n",
        "print(f\"Optimizing {best_model_name}...\\n\")\n",
        "print(f\"Initial RÂ²: {best_metrics['test_r2']:.4f}\")\n",
        "print(f\"Initial MAE: {best_metrics['test_mae']:.4f}\")\n",
        "print(\"\\nRunning hyperparameter optimization...\")\n",
        "\n",
        "# Optimize\n",
        "optimized_model, optimized_metrics, best_params = optimize_model(\n",
        "    model_type=best_model_type,\n",
        "    X=X_scaled,\n",
        "    y=y.values,\n",
        "    initial_metrics=best_metrics\n",
        ")\n",
        "\n",
        "print(f\"\\nOptimization complete!\")\n",
        "if best_params:\n",
        "    print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Optimization Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare before/after optimization\n",
        "if best_params:\n",
        "    comparison_data = {\n",
        "        'Metric': ['RÂ²', 'RMSE', 'MAE'],\n",
        "        'Before': [best_metrics['test_r2'], best_metrics['test_rmse'], best_metrics['test_mae']],\n",
        "        'After': [optimized_metrics['r2'], optimized_metrics['rmse'], optimized_metrics['mae']]\n",
        "    }\n",
        "    opt_df = pd.DataFrame(comparison_data)\n",
        "    opt_df['Improvement'] = opt_df['After'] - opt_df['Before']\n",
        "    \n",
        "    print(\"OPTIMIZATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(opt_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    \n",
        "    # Before/After comparison\n",
        "    x = np.arange(len(opt_df))\n",
        "    width = 0.35\n",
        "    \n",
        "    # Normalize for visualization (RÂ² is 0-1, RMSE and MAE are larger)\n",
        "    before_norm = [opt_df.loc[0, 'Before'], opt_df.loc[1, 'Before']/20, opt_df.loc[2, 'Before']/20]\n",
        "    after_norm = [opt_df.loc[0, 'After'], opt_df.loc[1, 'After']/20, opt_df.loc[2, 'After']/20]\n",
        "    \n",
        "    axes[0].bar(x - width/2, before_norm, width, label='Before', color='#e74c3c', edgecolor='black')\n",
        "    axes[0].bar(x + width/2, after_norm, width, label='After', color='#2ecc71', edgecolor='black')\n",
        "    axes[0].set_ylabel('Score (normalized)')\n",
        "    axes[0].set_title('Before vs After Optimization', fontweight='bold')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(opt_df['Metric'])\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Improvement\n",
        "    colors = ['#2ecc71' if imp > 0 else '#e74c3c' if imp < 0 else '#95a5a6' for imp in opt_df['Improvement']]\n",
        "    axes[1].bar(opt_df['Metric'], opt_df['Improvement'], color=colors, edgecolor='black')\n",
        "    axes[1].axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "    axes[1].set_ylabel('Improvement')\n",
        "    axes[1].set_title('Optimization Improvement (Positive is Better for RÂ², Negative for RMSE/MAE)', fontweight='bold')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(opt_df['Improvement']):\n",
        "        axes[1].text(i, v + 0.001 if v > 0 else v - 0.001, f'{v:+.4f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hyperparameters to optimize for this model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Final Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions with final model\n",
        "predictions = optimized_model.predict(X_scaled)\n",
        "errors = np.abs(y.values - predictions)\n",
        "residuals = y.values - predictions\n",
        "\n",
        "# Calculate final metrics\n",
        "final_r2 = r2_score(y.values, predictions)\n",
        "final_rmse = np.sqrt(mean_squared_error(y.values, predictions))\n",
        "final_mae = mean_absolute_error(y.values, predictions)\n",
        "\n",
        "# Accuracy thresholds\n",
        "within_5 = (errors <= 5).sum() / len(errors) * 100\n",
        "within_10 = (errors <= 10).sum() / len(errors) * 100\n",
        "within_15 = (errors <= 15).sum() / len(errors) * 100\n",
        "\n",
        "print(\"FINAL MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {best_model_name}\")\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  RÂ² Score: {final_r2:.4f} ({final_r2*100:.1f}% variance explained)\")\n",
        "print(f\"  RMSE: {final_rmse:.4f}\")\n",
        "print(f\"  MAE: {final_mae:.4f}\")\n",
        "print(f\"\\nAccuracy:\")\n",
        "print(f\"  Within Â±5 points: {within_5:.1f}%\")\n",
        "print(f\"  Within Â±10 points: {within_10:.1f}%\")\n",
        "print(f\"  Within Â±15 points: {within_15:.1f}%\")\n",
        "if best_params:\n",
        "    print(f\"\\nOptimized Parameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"  {param}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Final Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[0, 0].scatter(y.values, predictions, alpha=0.5, s=20, c=errors, cmap='RdYlGn_r')\n",
        "axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0, 0].set_xlabel('Actual Score')\n",
        "axes[0, 0].set_ylabel('Predicted Score')\n",
        "axes[0, 0].set_title('Actual vs Predicted Scores', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "cbar = plt.colorbar(axes[0, 0].collections[0], ax=axes[0, 0])\n",
        "cbar.set_label('Absolute Error')\n",
        "\n",
        "# Error distribution\n",
        "axes[0, 1].hist(errors, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(final_mae, color='red', linestyle='--', linewidth=2, label=f'MAE: {final_mae:.2f}')\n",
        "axes[0, 1].set_xlabel('Absolute Error')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Error Distribution', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Residual plot\n",
        "axes[1, 0].scatter(predictions, residuals, alpha=0.5, s=20)\n",
        "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1, 0].axhline(final_mae, color='orange', linestyle='--', linewidth=1, alpha=0.5)\n",
        "axes[1, 0].axhline(-final_mae, color='orange', linestyle='--', linewidth=1, alpha=0.5)\n",
        "axes[1, 0].set_xlabel('Predicted Score')\n",
        "axes[1, 0].set_ylabel('Residuals')\n",
        "axes[1, 0].set_title('Residual Plot', fontweight='bold')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Accuracy thresholds\n",
        "thresholds = ['Â±5 pts', 'Â±10 pts', 'Â±15 pts']\n",
        "accuracies = [within_5, within_10, within_15]\n",
        "colors = ['#3498db', '#9b59b6', '#1abc9c']\n",
        "axes[1, 1].bar(thresholds, accuracies, color=colors, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "axes[1, 1].set_title('Prediction Accuracy by Threshold', fontweight='bold')\n",
        "axes[1, 1].set_ylim([0, 100])\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[1, 1].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Feature Importance (Random Forest only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (if available)\n",
        "if hasattr(optimized_model, 'feature_importances_'):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': engineer.feature_names,\n",
        "        'importance': optimized_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
        "    print(\"=\"*60)\n",
        "    print(importance_df.head(15).to_string(index=False))\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Top 15 features\n",
        "    top_15 = importance_df.head(15)\n",
        "    axes[0].barh(range(len(top_15)), top_15['importance'].values, color='teal', edgecolor='black')\n",
        "    axes[0].set_yticks(range(len(top_15)))\n",
        "    axes[0].set_yticklabels(top_15['feature'].values)\n",
        "    axes[0].set_xlabel('Importance')\n",
        "    axes[0].set_title('Top 15 Most Important Features', fontweight='bold')\n",
        "    axes[0].invert_yaxis()\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Cumulative importance\n",
        "    cumsum = importance_df['importance'].cumsum() / importance_df['importance'].sum() * 100\n",
        "    axes[1].plot(range(1, len(cumsum)+1), cumsum.values, linewidth=2, color='purple')\n",
        "    axes[1].axhline(80, color='red', linestyle='--', linewidth=2, label='80% threshold')\n",
        "    axes[1].axhline(90, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
        "    axes[1].set_xlabel('Number of Features')\n",
        "    axes[1].set_ylabel('Cumulative Importance (%)')\n",
        "    axes[1].set_title('Cumulative Feature Importance', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    n_features_80 = (cumsum <= 80).sum() + 1\n",
        "    print(f\"\\n{n_features_80} features explain 80% of the model's decisions\")\n",
        "else:\n",
        "    print(\"Feature importance not available for this model type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete supervised model pipeline:\n",
        "\n",
        "1. **Data Preparation**: Loaded 9,660 movies with 31 engineered features\n",
        "2. **Model Comparison**: Trained and compared 3 models (Linear, Ridge, Random Forest)\n",
        "3. **Model Selection**: Selected best model based on RÂ² score\n",
        "4. **Hyperparameter Optimization**: Optimized best model using GridSearchCV\n",
        "5. **Final Performance**: Evaluated optimized model on all data\n",
        "\n",
        "The final model is saved and ready for production use!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
